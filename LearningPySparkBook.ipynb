{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "sc\n",
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chap2 - RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = sc.parallelize(\n",
    "    [('Amber', 22), ('Alfred', 23), ('Skye',4), ('Albert', 12), \n",
    "     ('Amber', 9)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:475"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.2 ms, sys: 12.7 ms, total: 40.9 ms\n",
      "Wall time: 22 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Amber', 22)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Amber', 'Alfred', 'Skye', 'Albert', 'Amber']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#map - applied to each element of the RDD - row\n",
    "data.map(lambda row: row[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reduce - processes data using a specified method: row[1]).reduce(lambda x, y: x + y)\n",
    "data.map(lambda row: row[1]).reduce(lambda x, y: x + y) #calc sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 - DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate our own JSON data \n",
    "#   This way we don't have to access the file system yet.\n",
    "stringJSONRDD = sc.parallelize((\"\"\" \n",
    "  { \"id\": \"123\",\n",
    "    \"name\": \"Katie\",\n",
    "    \"age\": 19,\n",
    "    \"eyeColor\": \"brown\"\n",
    "  }\"\"\",\n",
    "   \"\"\"{\n",
    "    \"id\": \"234\",\n",
    "    \"name\": \"Michael\",\n",
    "    \"age\": 22,\n",
    "    \"eyeColor\": \"green\"\n",
    "  }\"\"\", \n",
    "  \"\"\"{\n",
    "    \"id\": \"345\",\n",
    "    \"name\": \"Simone\",\n",
    "    \"age\": 23,\n",
    "    \"eyeColor\": \"blue\"\n",
    "  }\"\"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "swimmersJSON = spark.read.json(stringJSONRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(swimmersJSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create temporary table\n",
    "swimmersJSON.createOrReplaceTempView(\"swimmersJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+-------+\n",
      "|age|eyeColor| id|   name|\n",
      "+---+--------+---+-------+\n",
      "| 19|   brown|123|  Katie|\n",
      "| 22|   green|234|Michael|\n",
      "| 23|    blue|345| Simone|\n",
      "+---+--------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame API\n",
    "swimmersJSON.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=19, eyeColor=u'brown', id=u'123', name=u'Katie'),\n",
       " Row(age=22, eyeColor=u'green', id=u'234', name=u'Michael'),\n",
       " Row(age=23, eyeColor=u'blue', id=u'345', name=u'Simone')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SQL Query\n",
    "spark.sql(\"select * from swimmersJSON\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- eyeColor: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema\n",
    "swimmersJSON.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# Generate our own CSV data \n",
    "#   This way we don't have to access the file system yet.\n",
    "stringCSVRDD = sc.parallelize([(123, 'Katie', 19, 'brown'), (234, 'Michael', 22, 'green'), (345, 'Simone', 23, 'blue')])\n",
    "\n",
    "# The schema is encoded in a string, using StructType we define the schema using various pyspark.sql.types\n",
    "schemaString = \"id name age eyeColor\"\n",
    "schema = StructType([\n",
    "    StructField(\"id\", LongType(), True),    \n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", LongType(), True),\n",
    "    StructField(\"eyeColor\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Apply the schema to the RDD and Create DataFrame\n",
    "swimmers = spark.createDataFrame(stringCSVRDD, schema)\n",
    "\n",
    "# Creates a temporary view using the DataFrame\n",
    "swimmers.createOrReplaceTempView(\"swimmers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+--------+\n",
      "| id|   name|age|eyeColor|\n",
      "+---+-------+---+--------+\n",
      "|123|  Katie| 19|   brown|\n",
      "|234|Michael| 22|   green|\n",
      "|345| Simone| 23|    blue|\n",
      "+---+-------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Execute SQL Query and return the data\n",
    "spark.sql(\"select * from swimmers\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+--------+\n",
      "| id|   name|age|eyeColor|\n",
      "+---+-------+---+--------+\n",
      "|234|Michael| 22|   green|\n",
      "+---+-------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swimmers.filter('age==22').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swimmers.select(['name']).filter('age==22').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swimmers.select(['name']).filter(swimmers.age==22).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+--------+\n",
      "| id|  name|age|eyeColor|\n",
      "+---+------+---+--------+\n",
      "|123| Katie| 19|   brown|\n",
      "|345|Simone| 23|    blue|\n",
      "+---+------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "swimmers.where(col('eyeColor').like('b%')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|  name|eyeColor|\n",
      "+------+--------+\n",
      "| Katie|   brown|\n",
      "|Simone|    blue|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swimmers.select(\"name\", \"eyeColor\").filter(\"eyeColor like 'b%'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swimmers.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter4 - EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "        (1, 144.5, 5.9, 33, 'M'),\n",
    "        (2, 167.2, 5.4, 45, 'M'),\n",
    "        (3, 124.1, 5.2, 23, 'F'),\n",
    "        (4, 144.5, 5.9, 33, 'M'),\n",
    "        (5, 133.2, 5.7, 54, 'F'),\n",
    "        (3, 124.1, 5.2, 23, 'F'),\n",
    "        (5, 129.2, 5.3, 42, 'M'),\n",
    "    ], ['id', 'weight', 'height', 'age', 'gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+------+\n",
      "| id|weight|height|age|gender|\n",
      "+---+------+------+---+------+\n",
      "|  1| 144.5|   5.9| 33|     M|\n",
      "|  2| 167.2|   5.4| 45|     M|\n",
      "|  3| 124.1|   5.2| 23|     F|\n",
      "|  4| 144.5|   5.9| 33|     M|\n",
      "|  5| 133.2|   5.7| 54|     F|\n",
      "|  3| 124.1|   5.2| 23|     F|\n",
      "|  5| 129.2|   5.3| 42|     M|\n",
      "+---+------+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropDuplicates(subset=[i for i in df.columns if i not in ['id']])\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+\n",
      "|Count|countDistinct|\n",
      "+-----+-------------+\n",
      "|    5|            4|\n",
      "+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg(\n",
    "    fn.count('id').alias('Count'),\n",
    "    fn.countDistinct('id').alias('countDistinct'),\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+------+-------------+\n",
      "| id|weight|height|age|gender|          id2|\n",
      "+---+------+------+---+------+-------------+\n",
      "|  5| 133.2|   5.7| 54|     F|  25769803776|\n",
      "|  4| 144.5|   5.9| 33|     M| 171798691840|\n",
      "|  2| 167.2|   5.4| 45|     M| 592705486848|\n",
      "|  3| 124.1|   5.2| 23|     F|1236950581248|\n",
      "|  5| 129.2|   5.3| 42|     M|1365799600128|\n",
      "+---+------+------+---+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('id2', fn.monotonically_increasing_id()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----+------+------+\n",
      "| id|weight|height| age|gender|income|\n",
      "+---+------+------+----+------+------+\n",
      "|  1| 143.5|   5.6|  28|     M|100000|\n",
      "|  2| 167.2|   5.4|  45|     M|  null|\n",
      "|  3|  null|   5.2|null|  null|  null|\n",
      "|  4| 144.5|   5.9|  33|     M|  null|\n",
      "|  5| 133.2|   5.7|  54|     F|  null|\n",
      "|  6| 124.1|   5.2|null|     F|  null|\n",
      "|  7| 129.2|   5.3|  42|     M| 76000|\n",
      "+---+------+------+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_miss = spark.createDataFrame([\n",
    "        (1, 143.5, 5.6, 28,   'M',  100000),\n",
    "        (2, 167.2, 5.4, 45,   'M',  None),\n",
    "        (3, None , 5.2, None, None, None),\n",
    "        (4, 144.5, 5.9, 33,   'M',  None),\n",
    "        (5, 133.2, 5.7, 54,   'F',  None),\n",
    "        (6, 124.1, 5.2, None, 'F',  None),\n",
    "        (7, 129.2, 5.3, 42,   'M',  76000),\n",
    "    ], ['id', 'weight', 'height', 'age', 'gender', 'income'])\n",
    "df_miss.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----+------+------+-----------+\n",
      "| id|weight|height| age|gender|income|MissedCount|\n",
      "+---+------+------+----+------+------+-----------+\n",
      "|  1| 143.5|   5.6|  28|     M|100000|          0|\n",
      "|  2| 167.2|   5.4|  45|     M|  null|          1|\n",
      "|  3|  null|   5.2|null|  null|  null|          4|\n",
      "|  4| 144.5|   5.9|  33|     M|  null|          1|\n",
      "|  5| 133.2|   5.7|  54|     F|  null|          1|\n",
      "|  6| 124.1|   5.2|null|     F|  null|          2|\n",
      "|  7| 129.2|   5.3|  42|     M| 76000|          0|\n",
      "+---+------+------+----+------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#cast - Convert the column into type dataType\n",
    "df_miss.withColumn('MissedCount', sum(fn.isnull(col).cast('integer') for col in df_miss.columns)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+--------------+------------------+------------------+------------------+\n",
      "|id_missing|    weight_missing|height_missing|       age_missing|    gender_missing|    income_missing|\n",
      "+----------+------------------+--------------+------------------+------------------+------------------+\n",
      "|       0.0|0.1428571428571429|           0.0|0.2857142857142857|0.1428571428571429|0.7142857142857143|\n",
      "+----------+------------------+--------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#percentage of missing observations we see in each column\n",
    "df_miss.agg(*[\n",
    "    (1- fn.count(c)/fn.count('*')).alias(c+'_missing')\n",
    "    for c in df_miss.columns\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----+------+------+\n",
      "| id|weight|height| age|gender|income|\n",
      "+---+------+------+----+------+------+\n",
      "|  1| 143.5|   5.6|  28|     M|100000|\n",
      "|  2| 167.2|   5.4|  45|     M|  null|\n",
      "|  4| 144.5|   5.9|  33|     M|  null|\n",
      "|  5| 133.2|   5.7|  54|     F|  null|\n",
      "|  6| 124.1|   5.2|null|     F|  null|\n",
      "|  7| 129.2|   5.3|  42|     M| 76000|\n",
      "+---+------+------+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_miss.dropna(thresh=3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "means = df_miss.agg(*[\n",
    "    fn.mean(c).alias(c) for c in df_miss.columns if c !='gender'\n",
    "]).toPandas().to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age': 40.399999999999999,\n",
       " 'gender': 'unknown',\n",
       " 'height': 5.4714285714285724,\n",
       " 'id': 4.0,\n",
       " 'income': 88000.0,\n",
       " 'weight': 140.28333333333333}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means[0]['gender']='unknown'\n",
    "means[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------+---+-------+------+\n",
      "| id|       weight|height|age| gender|income|\n",
      "+---+-------------+------+---+-------+------+\n",
      "|  1|        143.5|   5.6| 28|      M|100000|\n",
      "|  2|        167.2|   5.4| 45|      M| 88000|\n",
      "|  3|140.283333333|   5.2| 40|unknown| 88000|\n",
      "|  4|        144.5|   5.9| 33|      M| 88000|\n",
      "|  5|        133.2|   5.7| 54|      F| 88000|\n",
      "|  6|        124.1|   5.2| 40|      F| 88000|\n",
      "|  7|        129.2|   5.3| 42|      M| 76000|\n",
      "+---+-------------+------+---+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_miss.fillna(means[0]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# outlier\n",
    "df_outliers = spark.createDataFrame([\n",
    "        (1, 143.5, 5.3, 28),\n",
    "        (2, 154.2, 5.5, 45),\n",
    "        (3, 342.3, 5.1, 99),\n",
    "        (4, 144.5, 5.5, 33),\n",
    "        (5, 133.2, 5.4, 54),\n",
    "        (6, 124.1, 5.1, 21),\n",
    "        (7, 129.2, 5.3, 42),\n",
    "    ], ['id', 'weight', 'height', 'age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+\n",
      "| id|weight|height|age|\n",
      "+---+------+------+---+\n",
      "|  1| 143.5|   5.3| 28|\n",
      "|  2| 154.2|   5.5| 45|\n",
      "|  3| 342.3|   5.1| 99|\n",
      "|  4| 144.5|   5.5| 33|\n",
      "|  5| 133.2|   5.4| 54|\n",
      "|  6| 124.1|   5.1| 21|\n",
      "|  7| 129.2|   5.3| 42|\n",
      "+---+------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_outliers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = ['weight', 'height', 'age']\n",
    "bounds = {}\n",
    "for col in cols:\n",
    "    quantiles = df_outliers.approxQuantile(col, [0.25, 0.75], 0.05)\n",
    "    IQR = quantiles[1] - quantiles[0]\n",
    "    bounds[col] = [quantiles[0] - 1.5 * IQR, quantiles[1] + 1.5 * IQR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age': [-11.0, 93.0],\n",
       " 'height': [4.499999999999999, 6.1000000000000005],\n",
       " 'weight': [91.69999999999999, 191.7]}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outliers = df_outliers\\\n",
    "    .select(*['id'] +[\n",
    "        ((df_outliers[c]<bounds[c][0])|(df_outliers[c]>bounds[c][1]))\\\n",
    "        .alias(c+'_out') for c in cols\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+-------+\n",
      "| id|weight_out|height_out|age_out|\n",
      "+---+----------+----------+-------+\n",
      "|  1|     false|     false|  false|\n",
      "|  2|     false|     false|  false|\n",
      "|  3|      true|     false|   true|\n",
      "|  4|     false|     false|  false|\n",
      "|  5|     false|     false|  false|\n",
      "|  6|     false|     false|  false|\n",
      "|  7|     false|     false|  false|\n",
      "+---+----------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outliers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+\n",
      "| id|weight|age|\n",
      "+---+------+---+\n",
      "|  3| 342.3| 99|\n",
      "+---+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_outliers = df_outliers.join(outliers, on='id')\n",
    "df_outliers.filter('weight_out').select('id', 'weight', 'age').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Chapter05 - MLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.types as typ\n",
    "\n",
    "labels = [\n",
    "    ('INFANT_ALIVE_AT_REPORT', typ.StringType()),\n",
    "    ('BIRTH_YEAR', typ.IntegerType()),\n",
    "    ('BIRTH_MONTH', typ.IntegerType()),\n",
    "    ('BIRTH_PLACE', typ.StringType()),\n",
    "    ('MOTHER_AGE_YEARS', typ.IntegerType()),\n",
    "    ('MOTHER_RACE_6CODE', typ.StringType()),\n",
    "    ('MOTHER_EDUCATION', typ.StringType()),\n",
    "    ('FATHER_COMBINED_AGE', typ.IntegerType()),\n",
    "    ('FATHER_EDUCATION', typ.StringType()),\n",
    "    ('MONTH_PRECARE_RECODE', typ.StringType()),\n",
    "    ('CIG_BEFORE', typ.IntegerType()),\n",
    "    ('CIG_1_TRI', typ.IntegerType()),\n",
    "    ('CIG_2_TRI', typ.IntegerType()),\n",
    "    ('CIG_3_TRI', typ.IntegerType()),\n",
    "    ('MOTHER_HEIGHT_IN', typ.IntegerType()),\n",
    "    ('MOTHER_BMI_RECODE', typ.IntegerType()),\n",
    "    ('MOTHER_PRE_WEIGHT', typ.IntegerType()),\n",
    "    ('MOTHER_DELIVERY_WEIGHT', typ.IntegerType()),\n",
    "    ('MOTHER_WEIGHT_GAIN', typ.IntegerType()),\n",
    "    ('DIABETES_PRE', typ.StringType()),\n",
    "    ('DIABETES_GEST', typ.StringType()),\n",
    "    ('HYP_TENS_PRE', typ.StringType()),\n",
    "    ('HYP_TENS_GEST', typ.StringType()),\n",
    "    ('PREV_BIRTH_PRETERM', typ.StringType()),\n",
    "    ('NO_RISK', typ.StringType()),\n",
    "    ('NO_INFECTIONS_REPORTED', typ.StringType()),\n",
    "    ('LABOR_IND', typ.StringType()),\n",
    "    ('LABOR_AUGM', typ.StringType()),\n",
    "    ('STEROIDS', typ.StringType()),\n",
    "    ('ANTIBIOTICS', typ.StringType()),\n",
    "    ('ANESTHESIA', typ.StringType()),\n",
    "    ('DELIV_METHOD_RECODE_COMB', typ.StringType()),\n",
    "    ('ATTENDANT_BIRTH', typ.StringType()),\n",
    "    ('APGAR_5', typ.IntegerType()),\n",
    "    ('APGAR_5_RECODE', typ.StringType()),\n",
    "    ('APGAR_10', typ.IntegerType()),\n",
    "    ('APGAR_10_RECODE', typ.StringType()),\n",
    "    ('INFANT_SEX', typ.StringType()),\n",
    "    ('OBSTETRIC_GESTATION_WEEKS', typ.IntegerType()),\n",
    "    ('INFANT_WEIGHT_GRAMS', typ.IntegerType()),\n",
    "    ('INFANT_ASSIST_VENTI', typ.StringType()),\n",
    "    ('INFANT_ASSIST_VENTI_6HRS', typ.StringType()),\n",
    "    ('INFANT_NICU_ADMISSION', typ.StringType()),\n",
    "    ('INFANT_SURFACANT', typ.StringType()),\n",
    "    ('INFANT_ANTIBIOTICS', typ.StringType()),\n",
    "    ('INFANT_SEIZURES', typ.StringType()),\n",
    "    ('INFANT_NO_ABNORMALITIES', typ.StringType()),\n",
    "    ('INFANT_ANCEPHALY', typ.StringType()),\n",
    "    ('INFANT_MENINGOMYELOCELE', typ.StringType()),\n",
    "    ('INFANT_LIMB_REDUCTION', typ.StringType()),\n",
    "    ('INFANT_DOWN_SYNDROME', typ.StringType()),\n",
    "    ('INFANT_SUSPECTED_CHROMOSOMAL_DISORDER', typ.StringType()),\n",
    "    ('INFANT_NO_CONGENITAL_ANOMALIES_CHECKED', typ.StringType()),\n",
    "    ('INFANT_BREASTFED', typ.StringType())\n",
    "]\n",
    "\n",
    "schema = typ.StructType([\n",
    "        typ.StructField(e[0], e[1], False) for e in labels\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "births = spark.read.csv('births_train.csv.gz',header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recode_dictionary = {\n",
    "    'YNU': {\n",
    "        'Y': 1,\n",
    "        'N': 0,\n",
    "        'U': 0\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = [\n",
    "    'INFANT_ALIVE_AT_REPORT', \n",
    "    'BIRTH_PLACE', \n",
    "    'MOTHER_AGE_YEARS', \n",
    "    'FATHER_COMBINED_AGE', \n",
    "    'CIG_BEFORE', \n",
    "    'CIG_1_TRI', \n",
    "    'CIG_2_TRI', \n",
    "    'CIG_3_TRI', \n",
    "    'MOTHER_HEIGHT_IN', \n",
    "    'MOTHER_PRE_WEIGHT', \n",
    "    'MOTHER_DELIVERY_WEIGHT', \n",
    "    'MOTHER_WEIGHT_GAIN', \n",
    "    'DIABETES_PRE', \n",
    "    'DIABETES_GEST', \n",
    "    'HYP_TENS_PRE', \n",
    "    'HYP_TENS_GEST', \n",
    "    'PREV_BIRTH_PRETERM'\n",
    "]\n",
    "\n",
    "births_trimmed = births.select(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func\n",
    "\n",
    "def recode(col, key):        \n",
    "    return recode_dictionary[key][col] \n",
    "\n",
    "def correct_cig(feat):\n",
    "    return func \\\n",
    "        .when(func.col(feat) != 99, func.col(feat))\\\n",
    "        .otherwise(0)\n",
    "\n",
    "rec_integer = func.udf(recode, typ.IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INFANT_ALIVE_AT_REPORT</th>\n",
       "      <th>BIRTH_PLACE</th>\n",
       "      <th>MOTHER_AGE_YEARS</th>\n",
       "      <th>FATHER_COMBINED_AGE</th>\n",
       "      <th>CIG_BEFORE</th>\n",
       "      <th>CIG_1_TRI</th>\n",
       "      <th>CIG_2_TRI</th>\n",
       "      <th>CIG_3_TRI</th>\n",
       "      <th>MOTHER_HEIGHT_IN</th>\n",
       "      <th>MOTHER_PRE_WEIGHT</th>\n",
       "      <th>MOTHER_DELIVERY_WEIGHT</th>\n",
       "      <th>MOTHER_WEIGHT_GAIN</th>\n",
       "      <th>DIABETES_PRE</th>\n",
       "      <th>DIABETES_GEST</th>\n",
       "      <th>HYP_TENS_PRE</th>\n",
       "      <th>HYP_TENS_GEST</th>\n",
       "      <th>PREV_BIRTH_PRETERM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>999</td>\n",
       "      <td>999</td>\n",
       "      <td>99</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>180</td>\n",
       "      <td>198</td>\n",
       "      <td>18</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>155</td>\n",
       "      <td>167</td>\n",
       "      <td>12</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  INFANT_ALIVE_AT_REPORT BIRTH_PLACE  MOTHER_AGE_YEARS  FATHER_COMBINED_AGE  \\\n",
       "0                      N           1                29                   99   \n",
       "1                      N           1                22                   29   \n",
       "2                      N           1                38                   40   \n",
       "\n",
       "   CIG_BEFORE  CIG_1_TRI  CIG_2_TRI  CIG_3_TRI  MOTHER_HEIGHT_IN  \\\n",
       "0          99         99         99         99                99   \n",
       "1           0          0          0          0                65   \n",
       "2           0          0          0          0                63   \n",
       "\n",
       "   MOTHER_PRE_WEIGHT  MOTHER_DELIVERY_WEIGHT  MOTHER_WEIGHT_GAIN DIABETES_PRE  \\\n",
       "0                999                     999                  99            N   \n",
       "1                180                     198                  18            N   \n",
       "2                155                     167                  12            N   \n",
       "\n",
       "  DIABETES_GEST HYP_TENS_PRE HYP_TENS_GEST PREV_BIRTH_PRETERM  \n",
       "0             N            N             N                  N  \n",
       "1             N            N             N                  N  \n",
       "2             N            N             N                  N  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "births_trimmed.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "births_transformed = births_trimmed \\\n",
    "    .withColumn('CIG_BEFORE', correct_cig('CIG_BEFORE'))\\\n",
    "    .withColumn('CIG_1_TRI', correct_cig('CIG_1_TRI'))\\\n",
    "    .withColumn('CIG_2_TRI', correct_cig('CIG_2_TRI'))\\\n",
    "    .withColumn('CIG_3_TRI', correct_cig('CIG_3_TRI'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INFANT_ALIVE_AT_REPORT</th>\n",
       "      <th>BIRTH_PLACE</th>\n",
       "      <th>MOTHER_AGE_YEARS</th>\n",
       "      <th>FATHER_COMBINED_AGE</th>\n",
       "      <th>CIG_BEFORE</th>\n",
       "      <th>CIG_1_TRI</th>\n",
       "      <th>CIG_2_TRI</th>\n",
       "      <th>CIG_3_TRI</th>\n",
       "      <th>MOTHER_HEIGHT_IN</th>\n",
       "      <th>MOTHER_PRE_WEIGHT</th>\n",
       "      <th>MOTHER_DELIVERY_WEIGHT</th>\n",
       "      <th>MOTHER_WEIGHT_GAIN</th>\n",
       "      <th>DIABETES_PRE</th>\n",
       "      <th>DIABETES_GEST</th>\n",
       "      <th>HYP_TENS_PRE</th>\n",
       "      <th>HYP_TENS_GEST</th>\n",
       "      <th>PREV_BIRTH_PRETERM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>999</td>\n",
       "      <td>999</td>\n",
       "      <td>99</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>180</td>\n",
       "      <td>198</td>\n",
       "      <td>18</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>155</td>\n",
       "      <td>167</td>\n",
       "      <td>12</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  INFANT_ALIVE_AT_REPORT BIRTH_PLACE  MOTHER_AGE_YEARS  FATHER_COMBINED_AGE  \\\n",
       "0                      N           1                29                   99   \n",
       "1                      N           1                22                   29   \n",
       "2                      N           1                38                   40   \n",
       "\n",
       "   CIG_BEFORE  CIG_1_TRI  CIG_2_TRI  CIG_3_TRI  MOTHER_HEIGHT_IN  \\\n",
       "0          99         99         99         99                99   \n",
       "1           0          0          0          0                65   \n",
       "2           0          0          0          0                63   \n",
       "\n",
       "   MOTHER_PRE_WEIGHT  MOTHER_DELIVERY_WEIGHT  MOTHER_WEIGHT_GAIN DIABETES_PRE  \\\n",
       "0                999                     999                  99            N   \n",
       "1                180                     198                  18            N   \n",
       "2                155                     167                  12            N   \n",
       "\n",
       "  DIABETES_GEST HYP_TENS_PRE HYP_TENS_GEST PREV_BIRTH_PRETERM  \n",
       "0             N            N             N                  N  \n",
       "1             N            N             N                  N  \n",
       "2             N            N             N                  N  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "births_trimmed.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = [(col.name, col.dataType) for col in births_trimmed.schema]\n",
    "\n",
    "YNU_cols = []\n",
    "\n",
    "for i, s in enumerate(cols):\n",
    "    if s[1] == typ.StringType():\n",
    "        dis = births.select(s[0]) \\\n",
    "            .distinct() \\\n",
    "            .rdd \\\n",
    "            .map(lambda row: row[0]) \\\n",
    "            .collect()\n",
    "\n",
    "        if 'Y' in dis:\n",
    "            YNU_cols.append(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('INFANT_ALIVE_AT_REPORT', StringType),\n",
       " ('BIRTH_PLACE', StringType),\n",
       " ('MOTHER_AGE_YEARS', IntegerType),\n",
       " ('FATHER_COMBINED_AGE', IntegerType),\n",
       " ('CIG_BEFORE', IntegerType),\n",
       " ('CIG_1_TRI', IntegerType),\n",
       " ('CIG_2_TRI', IntegerType),\n",
       " ('CIG_3_TRI', IntegerType),\n",
       " ('MOTHER_HEIGHT_IN', IntegerType),\n",
       " ('MOTHER_PRE_WEIGHT', IntegerType),\n",
       " ('MOTHER_DELIVERY_WEIGHT', IntegerType),\n",
       " ('MOTHER_WEIGHT_GAIN', IntegerType),\n",
       " ('DIABETES_PRE', StringType),\n",
       " ('DIABETES_GEST', StringType),\n",
       " ('HYP_TENS_PRE', StringType),\n",
       " ('HYP_TENS_GEST', StringType),\n",
       " ('PREV_BIRTH_PRETERM', StringType)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Y', u'U', u'N']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(INFANT_NICU_ADMISSION=u'Y', INFANT_NICU_ADMISSION_RECODE=1),\n",
       " Row(INFANT_NICU_ADMISSION=u'Y', INFANT_NICU_ADMISSION_RECODE=1),\n",
       " Row(INFANT_NICU_ADMISSION=u'U', INFANT_NICU_ADMISSION_RECODE=0),\n",
       " Row(INFANT_NICU_ADMISSION=u'N', INFANT_NICU_ADMISSION_RECODE=0),\n",
       " Row(INFANT_NICU_ADMISSION=u'U', INFANT_NICU_ADMISSION_RECODE=0)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "births.select([\n",
    "        'INFANT_NICU_ADMISSION', \n",
    "        rec_integer(\n",
    "            'INFANT_NICU_ADMISSION', func.lit('YNU')\n",
    "        ) \\\n",
    "        .alias('INFANT_NICU_ADMISSION_RECODE')]\n",
    "     ).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exprs_YNU = [\n",
    "    rec_integer(x, func.lit('YNU')).alias(x) \n",
    "    if x in YNU_cols \n",
    "    else x \n",
    "    for x in births_transformed.columns\n",
    "]\n",
    "\n",
    "births_transformed = births_transformed.select(exprs_YNU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+------------+-------------+------------------+\n",
      "|DIABETES_PRE|DIABETES_GEST|HYP_TENS_PRE|HYP_TENS_GEST|PREV_BIRTH_PRETERM|\n",
      "+------------+-------------+------------+-------------+------------------+\n",
      "|           0|            0|           0|            0|                 0|\n",
      "|           0|            0|           0|            0|                 0|\n",
      "|           0|            0|           0|            0|                 0|\n",
      "|           0|            0|           0|            0|                 1|\n",
      "|           0|            0|           0|            0|                 0|\n",
      "+------------+-------------+------------+-------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "births_transformed.select(YNU_cols[-5:]).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOTHER_AGE_YEARS: \t28.30 \t 6.08\n",
      "FATHER_COMBINED_AGE: \t44.55 \t 27.55\n",
      "CIG_BEFORE: \t1.43 \t 5.18\n",
      "CIG_1_TRI: \t0.91 \t 3.83\n",
      "CIG_2_TRI: \t0.70 \t 3.31\n",
      "CIG_3_TRI: \t0.58 \t 3.11\n",
      "MOTHER_HEIGHT_IN: \t65.12 \t 6.45\n",
      "MOTHER_PRE_WEIGHT: \t214.50 \t 210.21\n",
      "MOTHER_DELIVERY_WEIGHT: \t223.63 \t 180.01\n",
      "MOTHER_WEIGHT_GAIN: \t30.74 \t 26.23\n"
     ]
    }
   ],
   "source": [
    "import pyspark.mllib.stat as st\n",
    "import numpy as np\n",
    "\n",
    "numeric_cols = ['MOTHER_AGE_YEARS','FATHER_COMBINED_AGE',\n",
    "                'CIG_BEFORE','CIG_1_TRI','CIG_2_TRI','CIG_3_TRI',\n",
    "                'MOTHER_HEIGHT_IN','MOTHER_PRE_WEIGHT',\n",
    "                'MOTHER_DELIVERY_WEIGHT','MOTHER_WEIGHT_GAIN'\n",
    "               ]\n",
    "\n",
    "numeric_rdd = births_transformed\\\n",
    "                       .select(numeric_cols)\\\n",
    "                       .rdd \\\n",
    "                       .map(lambda row: [e for e in row])\n",
    "\n",
    "mllib_stats = st.Statistics.colStats(numeric_rdd)\n",
    "\n",
    "for col, m, v in zip(numeric_cols, \n",
    "                     mllib_stats.mean(), \n",
    "                     mllib_stats.variance()):\n",
    "    print('{0}: \\t{1:.2f} \\t {2:.2f}'.format(col, m, np.sqrt(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('INFANT_ALIVE_AT_REPORT', [(1, 23349), (0, 22080)])\n",
      "('BIRTH_PLACE', [(u'1', 44558), (u'4', 327), (u'3', 224), (u'2', 136), (u'7', 91), (u'5', 74), (u'6', 11), (u'9', 8)])\n",
      "('DIABETES_PRE', [(0, 44881), (1, 548)])\n",
      "('DIABETES_GEST', [(0, 43451), (1, 1978)])\n",
      "('HYP_TENS_PRE', [(0, 44348), (1, 1081)])\n",
      "('HYP_TENS_GEST', [(0, 43302), (1, 2127)])\n",
      "('PREV_BIRTH_PRETERM', [(0, 43088), (1, 2341)])\n"
     ]
    }
   ],
   "source": [
    "categorical_cols = [e for e in births_transformed.columns \n",
    "                    if e not in numeric_cols]\n",
    "\n",
    "categorical_rdd = births_transformed\\\n",
    "                       .select(categorical_cols)\\\n",
    "                       .rdd \\\n",
    "                       .map(lambda row: [e for e in row])\n",
    "            \n",
    "for i, col in enumerate(categorical_cols):\n",
    "    agg = categorical_rdd \\\n",
    "        .groupBy(lambda row: row[i]) \\\n",
    "        .map(lambda row: (row[0], len(row[1])))\n",
    "        \n",
    "    print(col, sorted(agg.collect(), \n",
    "                      key=lambda el: el[1], \n",
    "                      reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIG_BEFORE-to-CIG_1_TRI: 0.83\n",
      "CIG_BEFORE-to-CIG_2_TRI: 0.72\n",
      "CIG_BEFORE-to-CIG_3_TRI: 0.62\n",
      "CIG_1_TRI-to-CIG_BEFORE: 0.83\n",
      "CIG_1_TRI-to-CIG_2_TRI: 0.87\n",
      "CIG_1_TRI-to-CIG_3_TRI: 0.76\n",
      "CIG_2_TRI-to-CIG_BEFORE: 0.72\n",
      "CIG_2_TRI-to-CIG_1_TRI: 0.87\n",
      "CIG_2_TRI-to-CIG_3_TRI: 0.89\n",
      "CIG_3_TRI-to-CIG_BEFORE: 0.62\n",
      "CIG_3_TRI-to-CIG_1_TRI: 0.76\n",
      "CIG_3_TRI-to-CIG_2_TRI: 0.89\n",
      "MOTHER_PRE_WEIGHT-to-MOTHER_WEIGHT_GAIN: 0.65\n",
      "MOTHER_WEIGHT_GAIN-to-MOTHER_PRE_WEIGHT: 0.65\n"
     ]
    }
   ],
   "source": [
    "corrs = st.Statistics.corr(numeric_rdd)\n",
    "\n",
    "for i, el in enumerate(corrs > 0.6):\n",
    "    correlated = [\n",
    "        (numeric_cols[j], corrs[i][j]) \n",
    "        for j, e in enumerate(el) \n",
    "        if e == 1.0 and j != i]\n",
    "    \n",
    "    if len(correlated) > 0:\n",
    "        for e in correlated:\n",
    "            print('{0}-to-{1}: {2:.2f}' \\\n",
    "                  .format(numeric_cols[i], e[0], e[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_to_keep = [\n",
    "    'INFANT_ALIVE_AT_REPORT', \n",
    "    'BIRTH_PLACE', \n",
    "    'MOTHER_AGE_YEARS', \n",
    "    'FATHER_COMBINED_AGE', \n",
    "    'CIG_1_TRI', \n",
    "    'MOTHER_HEIGHT_IN', \n",
    "    'MOTHER_PRE_WEIGHT', \n",
    "    'DIABETES_PRE', \n",
    "    'DIABETES_GEST', \n",
    "    'HYP_TENS_PRE', \n",
    "    'HYP_TENS_GEST', \n",
    "    'PREV_BIRTH_PRETERM'\n",
    "]\n",
    "births_transformed = births_transformed.select([e for e in features_to_keep])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('BIRTH_PLACE', 0.0)\n",
      "('DIABETES_PRE', 0.0)\n",
      "('DIABETES_GEST', 0.0)\n",
      "('HYP_TENS_PRE', 0.0)\n",
      "('HYP_TENS_GEST', 0.0)\n",
      "('PREV_BIRTH_PRETERM', 0.0)\n"
     ]
    }
   ],
   "source": [
    "import pyspark.mllib.linalg as ln\n",
    "\n",
    "for cat in categorical_cols[1:]:\n",
    "    agg = births_transformed \\\n",
    "        .groupby('INFANT_ALIVE_AT_REPORT') \\\n",
    "        .pivot(cat) \\\n",
    "        .count()    \n",
    "\n",
    "    agg_rdd = agg \\\n",
    "        .rdd\\\n",
    "        .map(lambda row: (row[1:])) \\\n",
    "        .flatMap(lambda row: \n",
    "                 [0 if e == None else e for e in row]) \\\n",
    "        .collect()\n",
    "\n",
    "    row_length = len(agg.collect()[0]) - 1\n",
    "    agg = ln.Matrices.dense(row_length, 2, agg_rdd)\n",
    "    \n",
    "    test = st.Statistics.chiSqTest(agg)\n",
    "    print(cat, round(test.pValue, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an RDD of LabeledPoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.mllib.feature as ft\n",
    "import pyspark.mllib.regression as reg\n",
    "\n",
    "hashing = ft.HashingTF(7)\n",
    "\n",
    "births_hashed = births_transformed \\\n",
    "    .rdd \\\n",
    "    .map(lambda row: [\n",
    "            list(hashing.transform(row[1]).toArray()) \n",
    "                if col == 'BIRTH_PLACE' \n",
    "                else row[i] \n",
    "            for i, col \n",
    "            in enumerate(features_to_keep)]) \\\n",
    "    .map(lambda row: [[e] if type(e) == int else e \n",
    "                      for e in row]) \\\n",
    "    .map(lambda row: [item for sublist in row \n",
    "                      for item in sublist]) \\\n",
    "    .map(lambda row: reg.LabeledPoint(\n",
    "            row[0], \n",
    "            ln.Vectors.dense(row[1:]))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "births_train, births_test = births_hashed.randomSplit([0.6, 0.4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification \\\n",
    "    import LogisticRegressionWithLBFGS\n",
    "\n",
    "LR_Model = LogisticRegressionWithLBFGS \\\n",
    "    .train(births_train, iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LR_results = (\n",
    "        births_test.map(lambda row: row.label) \\\n",
    "        .zip(LR_Model \\\n",
    "             .predict(births_test\\\n",
    "                      .map(lambda row: row.features)))\n",
    "    ).map(lambda row: (row[0], row[1] * 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under PR: 0.85\n",
      "Area under ROC: 0.63\n"
     ]
    }
   ],
   "source": [
    "import pyspark.mllib.evaluation as ev\n",
    "LR_evaluation = ev.BinaryClassificationMetrics(LR_results)\n",
    "\n",
    "print('Area under PR: {0:.2f}' \\\n",
    "      .format(LR_evaluation.areaUnderPR))\n",
    "print('Area under ROC: {0:.2f}' \\\n",
    "      .format(LR_evaluation.areaUnderROC))\n",
    "LR_evaluation.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selector = ft.ChiSqSelector(4).fit(births_train)\n",
    "\n",
    "topFeatures_train = (\n",
    "        births_train.map(lambda row: row.label) \\\n",
    "        .zip(selector \\\n",
    "             .transform(births_train \\\n",
    "                        .map(lambda row: row.features)))\n",
    "    ).map(lambda row: reg.LabeledPoint(row[0], row[1]))\n",
    "\n",
    "topFeatures_test = (\n",
    "        births_test.map(lambda row: row.label) \\\n",
    "        .zip(selector \\\n",
    "             .transform(births_test \\\n",
    "                        .map(lambda row: row.features)))\n",
    "    ).map(lambda row: reg.LabeledPoint(row[0], row[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import RandomForest\n",
    "\n",
    "RF_model = RandomForest \\\n",
    "    .trainClassifier(data=topFeatures_train, \n",
    "                     numClasses=2, \n",
    "                     categoricalFeaturesInfo={}, \n",
    "                     numTrees=60,  \n",
    "                     featureSubsetStrategy='all',\n",
    "                     seed=666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under PR: 0.76\n",
      "Area under ROC: 0.60\n"
     ]
    }
   ],
   "source": [
    "RF_results = (\n",
    "        topFeatures_test.map(lambda row: row.label) \\\n",
    "        .zip(RF_model \\\n",
    "             .predict(topFeatures_test \\\n",
    "                      .map(lambda row: row.features)))\n",
    "    )\n",
    "\n",
    "RF_evaluation = ev.BinaryClassificationMetrics(RF_results)\n",
    "\n",
    "print('Area under PR: {0:.2f}' \\\n",
    "      .format(RF_evaluation.areaUnderPR))\n",
    "print('Area under ROC: {0:.2f}' \\\n",
    "      .format(RF_evaluation.areaUnderROC))\n",
    "RF_evaluation.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under PR: 0.87\n",
      "Area under ROC: 0.62\n"
     ]
    }
   ],
   "source": [
    "LR_Model_2 = LogisticRegressionWithLBFGS \\\n",
    "    .train(topFeatures_train, iterations=10)\n",
    "\n",
    "LR_results_2 = (\n",
    "        topFeatures_test.map(lambda row: row.label) \\\n",
    "        .zip(LR_Model_2 \\\n",
    "             .predict(topFeatures_test \\\n",
    "                      .map(lambda row: row.features)))\n",
    "    ).map(lambda row: (row[0], row[1] * 1.0))\n",
    "\n",
    "LR_evaluation_2 = ev.BinaryClassificationMetrics(LR_results_2)\n",
    "\n",
    "print('Area under PR: {0:.2f}' \\\n",
    "      .format(LR_evaluation_2.areaUnderPR))\n",
    "print('Area under ROC: {0:.2f}' \\\n",
    "      .format(LR_evaluation_2.areaUnderROC))\n",
    "LR_evaluation_2.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter6 - MLlib part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.types as typ\n",
    "\n",
    "labels = [\n",
    "    ('INFANT_ALIVE_AT_REPORT', typ.IntegerType()),\n",
    "    ('BIRTH_PLACE', typ.StringType()),\n",
    "    ('MOTHER_AGE_YEARS', typ.IntegerType()),\n",
    "    ('FATHER_COMBINED_AGE', typ.IntegerType()),\n",
    "    ('CIG_BEFORE', typ.IntegerType()),\n",
    "    ('CIG_1_TRI', typ.IntegerType()),\n",
    "    ('CIG_2_TRI', typ.IntegerType()),\n",
    "    ('CIG_3_TRI', typ.IntegerType()),\n",
    "    ('MOTHER_HEIGHT_IN', typ.IntegerType()),\n",
    "    ('MOTHER_PRE_WEIGHT', typ.IntegerType()),\n",
    "    ('MOTHER_DELIVERY_WEIGHT', typ.IntegerType()),\n",
    "    ('MOTHER_WEIGHT_GAIN', typ.IntegerType()),\n",
    "    ('DIABETES_PRE', typ.IntegerType()),\n",
    "    ('DIABETES_GEST', typ.IntegerType()),\n",
    "    ('HYP_TENS_PRE', typ.IntegerType()),\n",
    "    ('HYP_TENS_GEST', typ.IntegerType()),\n",
    "    ('PREV_BIRTH_PRETERM', typ.IntegerType())\n",
    "]\n",
    "\n",
    "schema = typ.StructType([\n",
    "    typ.StructField(e[0], e[1], False) for e in labels\n",
    "])\n",
    "\n",
    "births = spark.read.csv('births_transformed.csv.gz', \n",
    "                        header=True, \n",
    "                        schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INFANT_ALIVE_AT_REPORT</th>\n",
       "      <th>BIRTH_PLACE</th>\n",
       "      <th>MOTHER_AGE_YEARS</th>\n",
       "      <th>FATHER_COMBINED_AGE</th>\n",
       "      <th>CIG_BEFORE</th>\n",
       "      <th>CIG_1_TRI</th>\n",
       "      <th>CIG_2_TRI</th>\n",
       "      <th>CIG_3_TRI</th>\n",
       "      <th>MOTHER_HEIGHT_IN</th>\n",
       "      <th>MOTHER_PRE_WEIGHT</th>\n",
       "      <th>MOTHER_DELIVERY_WEIGHT</th>\n",
       "      <th>MOTHER_WEIGHT_GAIN</th>\n",
       "      <th>DIABETES_PRE</th>\n",
       "      <th>DIABETES_GEST</th>\n",
       "      <th>HYP_TENS_PRE</th>\n",
       "      <th>HYP_TENS_GEST</th>\n",
       "      <th>PREV_BIRTH_PRETERM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>999</td>\n",
       "      <td>999</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>180</td>\n",
       "      <td>198</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   INFANT_ALIVE_AT_REPORT BIRTH_PLACE  MOTHER_AGE_YEARS  FATHER_COMBINED_AGE  \\\n",
       "0                       0           1                29                   99   \n",
       "1                       0           1                22                   29   \n",
       "\n",
       "   CIG_BEFORE  CIG_1_TRI  CIG_2_TRI  CIG_3_TRI  MOTHER_HEIGHT_IN  \\\n",
       "0           0          0          0          0                99   \n",
       "1           0          0          0          0                65   \n",
       "\n",
       "   MOTHER_PRE_WEIGHT  MOTHER_DELIVERY_WEIGHT  MOTHER_WEIGHT_GAIN  \\\n",
       "0                999                     999                  99   \n",
       "1                180                     198                  18   \n",
       "\n",
       "   DIABETES_PRE  DIABETES_GEST  HYP_TENS_PRE  HYP_TENS_GEST  \\\n",
       "0             0              0             0              0   \n",
       "1             0              0             0              0   \n",
       "\n",
       "   PREV_BIRTH_PRETERM  \n",
       "0                   0  \n",
       "1                   0  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "births.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.ml.feature as ft\n",
    "\n",
    "births = births \\\n",
    "    .withColumn(       'BIRTH_PLACE_INT', \n",
    "                births['BIRTH_PLACE'] \\\n",
    "                    .cast(typ.IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ft.OneHotEncoder(\n",
    "    inputCol='BIRTH_PLACE_INT', \n",
    "    outputCol='BIRTH_PLACE_VEC')\n",
    "\n",
    "featuresCreator = ft.VectorAssembler(\n",
    "    inputCols=[\n",
    "        col[0] \n",
    "        for col \n",
    "        in labels[2:]] + \\\n",
    "    [encoder.getOutputCol()], \n",
    "    outputCol='features'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.feature.VectorAssembler"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(featuresCreator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.ml.classification as cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic = cl.LogisticRegression(\n",
    "    maxIter=10, \n",
    "    regParam=0.01, \n",
    "    labelCol='INFANT_ALIVE_AT_REPORT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "        encoder, \n",
    "        featuresCreator, \n",
    "        logistic\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "births_train, births_test = births \\\n",
    "    .randomSplit([0.7, 0.3], seed=666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- INFANT_ALIVE_AT_REPORT: integer (nullable = true)\n",
      " |-- BIRTH_PLACE: string (nullable = true)\n",
      " |-- MOTHER_AGE_YEARS: integer (nullable = true)\n",
      " |-- FATHER_COMBINED_AGE: integer (nullable = true)\n",
      " |-- CIG_BEFORE: integer (nullable = true)\n",
      " |-- CIG_1_TRI: integer (nullable = true)\n",
      " |-- CIG_2_TRI: integer (nullable = true)\n",
      " |-- CIG_3_TRI: integer (nullable = true)\n",
      " |-- MOTHER_HEIGHT_IN: integer (nullable = true)\n",
      " |-- MOTHER_PRE_WEIGHT: integer (nullable = true)\n",
      " |-- MOTHER_DELIVERY_WEIGHT: integer (nullable = true)\n",
      " |-- MOTHER_WEIGHT_GAIN: integer (nullable = true)\n",
      " |-- DIABETES_PRE: integer (nullable = true)\n",
      " |-- DIABETES_GEST: integer (nullable = true)\n",
      " |-- HYP_TENS_PRE: integer (nullable = true)\n",
      " |-- HYP_TENS_GEST: integer (nullable = true)\n",
      " |-- PREV_BIRTH_PRETERM: integer (nullable = true)\n",
      " |-- BIRTH_PLACE_INT: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "births.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(births_train)\n",
    "test_model = model.transform(births_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(INFANT_ALIVE_AT_REPORT=0, BIRTH_PLACE=u'1', MOTHER_AGE_YEARS=13, FATHER_COMBINED_AGE=99, CIG_BEFORE=0, CIG_1_TRI=0, CIG_2_TRI=0, CIG_3_TRI=0, MOTHER_HEIGHT_IN=66, MOTHER_PRE_WEIGHT=133, MOTHER_DELIVERY_WEIGHT=135, MOTHER_WEIGHT_GAIN=2, DIABETES_PRE=0, DIABETES_GEST=0, HYP_TENS_PRE=0, HYP_TENS_GEST=0, PREV_BIRTH_PRETERM=0, BIRTH_PLACE_INT=1, BIRTH_PLACE_VEC=SparseVector(9, {1: 1.0}), features=SparseVector(24, {0: 13.0, 1: 99.0, 6: 66.0, 7: 133.0, 8: 135.0, 9: 2.0, 16: 1.0}), rawPrediction=DenseVector([1.0573, -1.0573]), probability=DenseVector([0.7422, 0.2578]), prediction=0.0)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INFANT_ALIVE_AT_REPORT</th>\n",
       "      <th>BIRTH_PLACE</th>\n",
       "      <th>MOTHER_AGE_YEARS</th>\n",
       "      <th>FATHER_COMBINED_AGE</th>\n",
       "      <th>CIG_BEFORE</th>\n",
       "      <th>CIG_1_TRI</th>\n",
       "      <th>CIG_2_TRI</th>\n",
       "      <th>CIG_3_TRI</th>\n",
       "      <th>MOTHER_HEIGHT_IN</th>\n",
       "      <th>MOTHER_PRE_WEIGHT</th>\n",
       "      <th>...</th>\n",
       "      <th>DIABETES_GEST</th>\n",
       "      <th>HYP_TENS_PRE</th>\n",
       "      <th>HYP_TENS_GEST</th>\n",
       "      <th>PREV_BIRTH_PRETERM</th>\n",
       "      <th>BIRTH_PLACE_INT</th>\n",
       "      <th>BIRTH_PLACE_VEC</th>\n",
       "      <th>features</th>\n",
       "      <th>rawPrediction</th>\n",
       "      <th>probability</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>133</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>(13.0, 99.0, 0.0, 0.0, 0.0, 0.0, 66.0, 133.0, ...</td>\n",
       "      <td>[1.05726758028, -1.05726758028]</td>\n",
       "      <td>[0.742168030173, 0.257831969827]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   INFANT_ALIVE_AT_REPORT BIRTH_PLACE  MOTHER_AGE_YEARS  FATHER_COMBINED_AGE  \\\n",
       "0                       0           1                13                   99   \n",
       "\n",
       "   CIG_BEFORE  CIG_1_TRI  CIG_2_TRI  CIG_3_TRI  MOTHER_HEIGHT_IN  \\\n",
       "0           0          0          0          0                66   \n",
       "\n",
       "   MOTHER_PRE_WEIGHT    ...      DIABETES_GEST  HYP_TENS_PRE  HYP_TENS_GEST  \\\n",
       "0                133    ...                  0             0              0   \n",
       "\n",
       "   PREV_BIRTH_PRETERM  BIRTH_PLACE_INT  \\\n",
       "0                   0                1   \n",
       "\n",
       "                                 BIRTH_PLACE_VEC  \\\n",
       "0  (0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)   \n",
       "\n",
       "                                            features  \\\n",
       "0  (13.0, 99.0, 0.0, 0.0, 0.0, 0.0, 66.0, 133.0, ...   \n",
       "\n",
       "                     rawPrediction                       probability  \\\n",
       "0  [1.05726758028, -1.05726758028]  [0.742168030173, 0.257831969827]   \n",
       "\n",
       "  prediction  \n",
       "0        0.0  \n",
       "\n",
       "[1 rows x 23 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model.limit(1).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.74013018471\n",
      "0.713935434237\n"
     ]
    }
   ],
   "source": [
    "import pyspark.ml.evaluation as ev\n",
    "\n",
    "evaluator = ev.BinaryClassificationEvaluator(\n",
    "    rawPredictionCol='probability', \n",
    "    labelCol='INFANT_ALIVE_AT_REPORT')\n",
    "\n",
    "print(evaluator.evaluate(test_model, \n",
    "     {evaluator.metricName: 'areaUnderROC'}))\n",
    "print(evaluator.evaluate(test_model, {evaluator.metricName: 'areaUnderPR'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter hyper-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.ml.tuning as tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic = cl.LogisticRegression(\n",
    "    labelCol='INFANT_ALIVE_AT_REPORT')\n",
    "\n",
    "grid = tune.ParamGridBuilder() \\\n",
    "    .addGrid(logistic.maxIter,  \n",
    "             [2, 10, 50]) \\\n",
    "    .addGrid(logistic.regParam, \n",
    "             [0.01, 0.05, 0.3]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluator = ev.BinaryClassificationEvaluator(\n",
    "    rawPredictionCol='probability', \n",
    "    labelCol='INFANT_ALIVE_AT_REPORT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = tune.CrossValidator(\n",
    "    estimator=logistic, \n",
    "    estimatorParamMaps=grid, \n",
    "    evaluator=evaluator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[encoder,featuresCreator])\n",
    "data_transformer = pipeline.fit(births_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvModel = cv.fit(data_transformer.transform(births_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.740495980331\n",
      "0.715797110849\n"
     ]
    }
   ],
   "source": [
    "data_train = data_transformer \\\n",
    "    .transform(births_test)\n",
    "results = cvModel.transform(data_train)\n",
    "\n",
    "print(evaluator.evaluate(results, \n",
    "     {evaluator.metricName: 'areaUnderROC'}))\n",
    "print(evaluator.evaluate(results, \n",
    "     {evaluator.metricName: 'areaUnderPR'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'regParam': 0.01}, {'maxIter': 50}], 0.7386231679256547)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = [\n",
    "    (\n",
    "        [\n",
    "            {key.name: paramValue} \n",
    "            for key, paramValue \n",
    "            in zip(\n",
    "                params.keys(), \n",
    "                params.values())\n",
    "        ], metric\n",
    "    ) \n",
    "    for params, metric \n",
    "    in zip(\n",
    "        cvModel.getEstimatorParamMaps(), \n",
    "        cvModel.avgMetrics\n",
    "    )\n",
    "]\n",
    "\n",
    "sorted(results, \n",
    "       key=lambda el: el[1], \n",
    "       reverse=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([{'regParam': 0.01}, {'maxIter': 50}], 0.7386231679256547),\n",
       " ([{'regParam': 0.01}, {'maxIter': 10}], 0.7381668960301381),\n",
       " ([{'regParam': 0.05}, {'maxIter': 10}], 0.7334243774610238),\n",
       " ([{'regParam': 0.05}, {'maxIter': 50}], 0.7332344334808222),\n",
       " ([{'regParam': 0.3}, {'maxIter': 10}], 0.7219557301690844),\n",
       " ([{'regParam': 0.3}, {'maxIter': 50}], 0.7189548844726206),\n",
       " ([{'regParam': 0.3}, {'maxIter': 2}], 0.6972576298932318),\n",
       " ([{'regParam': 0.05}, {'maxIter': 2}], 0.6965519395867678),\n",
       " ([{'regParam': 0.01}, {'maxIter': 2}], 0.696435989445255)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(results, \n",
    "       key=lambda el: el[1], \n",
    "       reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_data = spark.createDataFrame([\n",
    "    ['''Machine learning can be applied to a wide variety \n",
    "        of data types, such as vectors, text, images, and \n",
    "        structured data. This API adopts the DataFrame from \n",
    "        Spark SQL in order to support a variety of data types.'''],\n",
    "    ['''DataFrame supports many basic and structured types; \n",
    "        see the Spark SQL datatype reference for a list of \n",
    "        supported types. In addition to the types listed in \n",
    "        the Spark SQL guide, DataFrame can use ML Vector types.'''],\n",
    "    ['''A DataFrame can be created either implicitly or \n",
    "        explicitly from a regular RDD. See the code examples \n",
    "        below and the Spark SQL programming guide for examples.'''],\n",
    "    ['''Columns in a DataFrame are named. The code examples \n",
    "        below use names such as \"text,\" \"features,\" and \"label.\"''']\n",
    "], ['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               input|\n",
      "+--------------------+\n",
      "|Machine learning ...|\n",
      "|DataFrame support...|\n",
      "|A DataFrame can b...|\n",
      "|Columns in a Data...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_data.show()#toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = ft.RegexTokenizer(\n",
    "    inputCol='input', \n",
    "    outputCol='input_arr', \n",
    "    pattern='\\s+|[,.\\\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(input_arr=[u'machine', u'learning', u'can', u'be', u'applied', u'to', u'a', u'wide', u'variety', u'of', u'data', u'types', u'such', u'as', u'vectors', u'text', u'images', u'and', u'structured', u'data', u'this', u'api', u'adopts', u'the', u'dataframe', u'from', u'spark', u'sql', u'in', u'order', u'to', u'support', u'a', u'variety', u'of', u'data', u'types'])]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = tokenizer \\\n",
    "    .transform(text_data) \\\n",
    "    .select('input_arr') \n",
    "\n",
    "tok.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = ft.StopWordsRemover(\n",
    "    inputCol=tokenizer.getOutputCol(), \n",
    "    outputCol='input_stop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(input_stop=[u'machine', u'learning', u'applied', u'wide', u'variety', u'data', u'types', u'vectors', u'text', u'images', u'structured', u'data', u'api', u'adopts', u'dataframe', u'spark', u'sql', u'order', u'support', u'variety', u'data', u'types'])]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.transform(tok).select('input_stop').take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngram = ft.NGram(n=2, \n",
    "    inputCol=stopwords.getOutputCol(), \n",
    "    outputCol=\"nGrams\")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, stopwords, ngram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(nGrams=[u'machine learning', u'learning applied', u'applied wide', u'wide variety', u'variety data', u'data types', u'types vectors', u'vectors text', u'text images', u'images structured', u'structured data', u'data api', u'api adopts', u'adopts dataframe', u'dataframe spark', u'spark sql', u'sql order', u'order support', u'support variety', u'variety data', u'data types'])]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ngram = pipeline \\\n",
    "    .fit(text_data) \\\n",
    "    .transform(text_data)\n",
    "    \n",
    "data_ngram.select('nGrams').take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.arange(0, 100)\n",
    "x = x / 100.0 * np.pi * 4\n",
    "y = x * np.sin(x / 1.764) + 20.1234\n",
    "\n",
    "schema = typ.StructType([\n",
    "    typ.StructField('continuous_var', \n",
    "                    typ.DoubleType(), \n",
    "                    False\n",
    "   )\n",
    "])\n",
    "\n",
    "data = spark.createDataFrame([[float(e), ] for e in y], schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "discretizer = ft.QuantileDiscretizer(\n",
    "    numBuckets=5, \n",
    "    inputCol='continuous_var', \n",
    "    outputCol='discretized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_discretized = discretizer.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+\n",
      "|    continuous_var|discretized|\n",
      "+------------------+-----------+\n",
      "|           20.1234|        2.0|\n",
      "|20.132344452369832|        2.0|\n",
      "|20.159087064491775|        2.0|\n",
      "|20.203356291885854|        2.0|\n",
      "| 20.26470185735763|        2.0|\n",
      "|20.342498180090526|        2.0|\n",
      "|  20.4359491438498|        2.0|\n",
      "|20.544094172020312|        2.0|\n",
      "|20.665815568330437|        2.0|\n",
      "|20.799847073505322|        2.0|\n",
      "|  20.9447835797997|        2.0|\n",
      "| 21.09909193743627|        2.0|\n",
      "|21.261122779470593|        3.0|\n",
      "| 21.42912328456607|        3.0|\n",
      "| 21.60125079063745|        3.0|\n",
      "|21.775587166351258|        3.0|\n",
      "|21.950153842094366|        3.0|\n",
      "|22.122927397273514|        3.0|\n",
      "|22.291855596719525|        3.0|\n",
      "|22.454873765567744|        3.0|\n",
      "+------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_discretized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(discretized=0.0, avg(continuous_var)=12.314360733007911),\n",
       " Row(discretized=1.0, avg(continuous_var)=16.182704573335364),\n",
       " Row(discretized=2.0, avg(continuous_var)=20.36223167245682),\n",
       " Row(discretized=3.0, avg(continuous_var)=22.127093889410954),\n",
       " Row(discretized=4.0, avg(continuous_var)=24.333666689963955)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_discretized \\\n",
    "    .groupby('discretized')\\\n",
    "    .mean('continuous_var')\\\n",
    "    .sort('discretized')\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = ft.VectorAssembler(\n",
    "    inputCols=['continuous_var'], \n",
    "    outputCol= 'continuous_vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalizer = ft.StandardScaler(\n",
    "    inputCol=vectorizer.getOutputCol(), \n",
    "    outputCol='normalized', \n",
    "    withMean=True,\n",
    "    withStd=True\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[vectorizer, normalizer])\n",
    "data_standardized = pipeline.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func\n",
    "\n",
    "births = births.withColumn(\n",
    "    'INFANT_ALIVE_AT_REPORT', \n",
    "    func.col('INFANT_ALIVE_AT_REPORT').cast(typ.DoubleType())\n",
    ")\n",
    "\n",
    "births_train, births_test = births \\\n",
    "    .randomSplit([0.7, 0.3], seed=666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = cl.RandomForestClassifier(\n",
    "    numTrees=5, \n",
    "    maxDepth=5, \n",
    "    labelCol='INFANT_ALIVE_AT_REPORT')\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages=[\n",
    "        encoder,\n",
    "        featuresCreator, \n",
    "        classifier])\n",
    "\n",
    "model = pipeline.fit(births_train)\n",
    "test = model.transform(births_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.760156569968\n",
      "0.701576252483\n"
     ]
    }
   ],
   "source": [
    "evaluator = ev.BinaryClassificationEvaluator(\n",
    "    labelCol='INFANT_ALIVE_AT_REPORT')\n",
    "print(evaluator.evaluate(test, \n",
    "    {evaluator.metricName: \"areaUnderROC\"}))\n",
    "print(evaluator.evaluate(test, \n",
    "    {evaluator.metricName: \"areaUnderPR\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = cl.RandomForestClassifier(\n",
    "    numTrees=15, \n",
    "    maxDepth=5, \n",
    "    labelCol='INFANT_ALIVE_AT_REPORT')\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages=[\n",
    "        encoder,\n",
    "        featuresCreator, \n",
    "        classifier])\n",
    "\n",
    "model = pipeline.fit(births_train)\n",
    "test = model.transform(births_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.772368970018\n",
      "0.752500062371\n"
     ]
    }
   ],
   "source": [
    "evaluator = ev.BinaryClassificationEvaluator(\n",
    "    labelCol='INFANT_ALIVE_AT_REPORT')\n",
    "print(evaluator.evaluate(test, \n",
    "    {evaluator.metricName: \"areaUnderROC\"}))\n",
    "print(evaluator.evaluate(test, \n",
    "    {evaluator.metricName: \"areaUnderPR\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.757757793199\n",
      "0.780444144658\n"
     ]
    }
   ],
   "source": [
    "classifier = cl.DecisionTreeClassifier(\n",
    "    maxDepth=5, \n",
    "    labelCol='INFANT_ALIVE_AT_REPORT')\n",
    "pipeline = Pipeline(stages=[\n",
    "        encoder,\n",
    "        featuresCreator, \n",
    "        classifier]\n",
    ")\n",
    "\n",
    "model = pipeline.fit(births_train)\n",
    "test = model.transform(births_test)\n",
    "\n",
    "evaluator = ev.BinaryClassificationEvaluator(\n",
    "    labelCol='INFANT_ALIVE_AT_REPORT')\n",
    "print(evaluator.evaluate(test, \n",
    "     {evaluator.metricName: \"areaUnderROC\"}))\n",
    "print(evaluator.evaluate(test, \n",
    "     {evaluator.metricName: \"areaUnderPR\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.ml.clustering as clus\n",
    "\n",
    "kmeans = clus.KMeans(k = 5, \n",
    "    featuresCol='features')\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "        encoder,\n",
    "        featuresCreator, \n",
    "        kmeans]\n",
    ")\n",
    "\n",
    "model = pipeline.fit(births_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(prediction=1, avg(MOTHER_HEIGHT_IN)=83.91154791154791, count(1)=407),\n",
       " Row(prediction=3, avg(MOTHER_HEIGHT_IN)=67.69473684210526, count(1)=475),\n",
       " Row(prediction=4, avg(MOTHER_HEIGHT_IN)=63.90993407084591, count(1)=8949),\n",
       " Row(prediction=2, avg(MOTHER_HEIGHT_IN)=66.64658634538152, count(1)=249),\n",
       " Row(prediction=0, avg(MOTHER_HEIGHT_IN)=65.3889041472123, count(1)=3641)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = model.transform(births_test)\n",
    "\n",
    "test \\\n",
    "    .groupBy('prediction') \\\n",
    "    .agg({\n",
    "        '*': 'count', \n",
    "        'MOTHER_HEIGHT_IN': 'avg'\n",
    "    }).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_data = spark.createDataFrame([\n",
    "    ['''To make a computer do anything, you have to write a \n",
    "    computer program. To write a computer program, you have \n",
    "    to tell the computer, step by step, exactly what you want \n",
    "    it to do. The computer then \"executes\" the program, \n",
    "    following each step mechanically, to accomplish the end \n",
    "    goal. When you are telling the computer what to do, you \n",
    "    also get to choose how it's going to do it. That's where \n",
    "    computer algorithms come in. The algorithm is the basic \n",
    "    technique used to get the job done. Let's follow an \n",
    "    example to help get an understanding of the algorithm \n",
    "    concept.'''],\n",
    "    ['''Laptop computers use batteries to run while not \n",
    "    connected to mains. When we overcharge or overheat \n",
    "    lithium ion batteries, the materials inside start to \n",
    "    break down and produce bubbles of oxygen, carbon dioxide, \n",
    "    and other gases. Pressure builds up, and the hot battery \n",
    "    swells from a rectangle into a pillow shape. Sometimes \n",
    "    the phone involved will operate afterwards. Other times \n",
    "    it will die. And occasionally—kapow! To see what's \n",
    "    happening inside the battery when it swells, the CLS team \n",
    "    used an x-ray technology called computed tomography.'''],\n",
    "    ['''This technology describes a technique where touch \n",
    "    sensors can be placed around any side of a device \n",
    "    allowing for new input sources. The patent also notes \n",
    "    that physical buttons (such as the volume controls) could \n",
    "    be replaced by these embedded touch sensors. In essence \n",
    "    Apple could drop the current buttons and move towards \n",
    "    touch-enabled areas on the device for the existing UI. It \n",
    "    could also open up areas for new UI paradigms, such as \n",
    "    using the back of the smartphone for quick scrolling or \n",
    "    page turning.'''],\n",
    "    ['''The National Park Service is a proud protector of \n",
    "    America’s lands. Preserving our land not only safeguards \n",
    "    the natural environment, but it also protects the \n",
    "    stories, cultures, and histories of our ancestors. As we \n",
    "    face the increasingly dire consequences of climate \n",
    "    change, it is imperative that we continue to expand \n",
    "    America’s protected lands under the oversight of the \n",
    "    National Park Service. Doing so combats climate change \n",
    "    and allows all American’s to visit, explore, and learn \n",
    "    from these treasured places for generations to come. It \n",
    "    is critical that President Obama acts swiftly to preserve \n",
    "    land that is at risk of external threats before the end \n",
    "    of his term as it has become blatantly clear that the \n",
    "    next administration will not hold the same value for our \n",
    "    environment over the next four years.'''],\n",
    "    ['''The National Park Foundation, the official charitable \n",
    "    partner of the National Park Service, enriches America’s \n",
    "    national parks and programs through the support of \n",
    "    private citizens, park lovers, stewards of nature, \n",
    "    history enthusiasts, and wilderness adventurers. \n",
    "    Chartered by Congress in 1967, the Foundation grew out of \n",
    "    a legacy of park protection that began over a century \n",
    "    ago, when ordinary citizens took action to establish and \n",
    "    protect our national parks. Today, the National Park \n",
    "    Foundation carries on the tradition of early park \n",
    "    advocates, big thinkers, doers and dreamers—from John \n",
    "    Muir and Ansel Adams to President Theodore Roosevelt.'''],\n",
    "    ['''Australia has over 500 national parks. Over 28 \n",
    "    million hectares of land is designated as national \n",
    "    parkland, accounting for almost four per cent of \n",
    "    Australia's land areas. In addition, a further six per \n",
    "    cent of Australia is protected and includes state \n",
    "    forests, nature parks and conservation reserves.National \n",
    "    parks are usually large areas of land that are protected \n",
    "    because they have unspoilt landscapes and a diverse \n",
    "    number of native plants and animals. This means that \n",
    "    commercial activities such as farming are prohibited and \n",
    "    human activity is strictly monitored.''']\n",
    "], ['documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = ft.RegexTokenizer(\n",
    "    inputCol='documents', \n",
    "    outputCol='input_arr', \n",
    "    pattern='\\s+|[,.\\\"]')\n",
    "\n",
    "stopwords = ft.StopWordsRemover(\n",
    "    inputCol=tokenizer.getOutputCol(), \n",
    "    outputCol='input_stop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(input_indexed=SparseVector(262, {2: 7.0, 5: 1.0, 8: 3.0, 12: 3.0, 13: 3.0, 15: 1.0, 17: 2.0, 22: 1.0, 26: 1.0, 29: 1.0, 34: 2.0, 57: 1.0, 95: 1.0, 99: 1.0, 107: 1.0, 111: 1.0, 112: 1.0, 115: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 140: 1.0, 149: 1.0, 151: 1.0, 153: 1.0, 163: 1.0, 167: 1.0, 176: 1.0, 186: 1.0, 189: 1.0, 233: 1.0, 240: 1.0})),\n",
       " Row(input_indexed=SparseVector(262, {18: 2.0, 22: 1.0, 24: 2.0, 30: 1.0, 32: 2.0, 36: 2.0, 46: 1.0, 50: 1.0, 53: 1.0, 54: 1.0, 56: 1.0, 59: 1.0, 60: 1.0, 62: 1.0, 73: 1.0, 76: 1.0, 86: 1.0, 88: 1.0, 92: 1.0, 98: 1.0, 100: 1.0, 102: 1.0, 110: 1.0, 131: 1.0, 139: 1.0, 142: 1.0, 143: 1.0, 150: 1.0, 157: 1.0, 161: 1.0, 165: 1.0, 166: 1.0, 170: 1.0, 171: 1.0, 192: 1.0, 194: 1.0, 195: 1.0, 204: 1.0, 205: 1.0, 209: 1.0, 211: 1.0, 222: 1.0, 229: 1.0, 238: 1.0, 241: 1.0, 242: 1.0, 256: 1.0, 258: 1.0}))]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stringIndexer = ft.CountVectorizer(\n",
    "    inputCol=stopwords.getOutputCol(), \n",
    "    outputCol=\"input_indexed\")\n",
    "\n",
    "tokenized = stopwords \\\n",
    "    .transform(\n",
    "        tokenizer\\\n",
    "            .transform(text_data)\n",
    "    )\n",
    "    \n",
    "stringIndexer \\\n",
    "    .fit(tokenized)\\\n",
    "    .transform(tokenized)\\\n",
    "    .select('input_indexed')\\\n",
    "    .take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clustering = clus.LDA(k=3, optimizer='online', featuresCol=stringIndexer.getOutputCol())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[\n",
    "        tokenizer, \n",
    "        stopwords,\n",
    "        stringIndexer, \n",
    "        clustering]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(topicDistribution=DenseVector([0.9849, 0.0082, 0.0069])),\n",
       " Row(topicDistribution=DenseVector([0.0066, 0.0078, 0.9855])),\n",
       " Row(topicDistribution=DenseVector([0.0065, 0.0079, 0.9856])),\n",
       " Row(topicDistribution=DenseVector([0.0047, 0.9907, 0.0046])),\n",
       " Row(topicDistribution=DenseVector([0.0054, 0.9892, 0.0054])),\n",
       " Row(topicDistribution=DenseVector([0.0064, 0.9873, 0.0063]))]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics = pipeline \\\n",
    "    .fit(text_data) \\\n",
    "    .transform(text_data)\n",
    "\n",
    "topics.select('topicDistribution').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>documents</th>\n",
       "      <th>input_arr</th>\n",
       "      <th>input_stop</th>\n",
       "      <th>input_indexed</th>\n",
       "      <th>topicDistribution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>To make a computer do anything, you have to wr...</td>\n",
       "      <td>[to, make, a, computer, do, anything, you, hav...</td>\n",
       "      <td>[make, computer, anything, write, computer, pr...</td>\n",
       "      <td>(0.0, 0.0, 7.0, 0.0, 0.0, 1.0, 0.0, 3.0, 3.0, ...</td>\n",
       "      <td>[0.984877244203, 0.00820553810273, 0.006917217...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Laptop computers use batteries to run while no...</td>\n",
       "      <td>[laptop, computers, use, batteries, to, run, w...</td>\n",
       "      <td>[laptop, computers, use, batteries, run, conne...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.00664519926157, 0.00780682374983, 0.9855479...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This technology describes a technique where to...</td>\n",
       "      <td>[this, technology, describes, a, technique, wh...</td>\n",
       "      <td>[technology, describes, technique, touch, sens...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.00653605489611, 0.00785412651842, 0.9856098...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           documents  \\\n",
       "0  To make a computer do anything, you have to wr...   \n",
       "1  Laptop computers use batteries to run while no...   \n",
       "2  This technology describes a technique where to...   \n",
       "\n",
       "                                           input_arr  \\\n",
       "0  [to, make, a, computer, do, anything, you, hav...   \n",
       "1  [laptop, computers, use, batteries, to, run, w...   \n",
       "2  [this, technology, describes, a, technique, wh...   \n",
       "\n",
       "                                          input_stop  \\\n",
       "0  [make, computer, anything, write, computer, pr...   \n",
       "1  [laptop, computers, use, batteries, run, conne...   \n",
       "2  [technology, describes, technique, touch, sens...   \n",
       "\n",
       "                                       input_indexed  \\\n",
       "0  (0.0, 0.0, 7.0, 0.0, 0.0, 1.0, 0.0, 3.0, 3.0, ...   \n",
       "1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  (0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                   topicDistribution  \n",
       "0  [0.984877244203, 0.00820553810273, 0.006917217...  \n",
       "1  [0.00664519926157, 0.00780682374983, 0.9855479...  \n",
       "2  [0.00653605489611, 0.00785412651842, 0.9856098...  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = ['MOTHER_AGE_YEARS','MOTHER_HEIGHT_IN',\n",
    "            'MOTHER_PRE_WEIGHT','DIABETES_PRE',\n",
    "            'DIABETES_GEST','HYP_TENS_PRE', \n",
    "            'HYP_TENS_GEST', 'PREV_BIRTH_PRETERM',\n",
    "            'CIG_BEFORE','CIG_1_TRI', 'CIG_2_TRI', \n",
    "            'CIG_3_TRI'\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featuresCreator = ft.VectorAssembler(\n",
    "    inputCols=[col for col in features[1:]], \n",
    "    outputCol='features'\n",
    ")\n",
    "\n",
    "selector = ft.ChiSqSelector(\n",
    "    numTopFeatures=6, \n",
    "    outputCol=\"selectedFeatures\", \n",
    "    labelCol='MOTHER_WEIGHT_GAIN'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.ml.regression as reg\n",
    "\n",
    "regressor = reg.GBTRegressor(\n",
    "    maxIter=15, \n",
    "    maxDepth=3,\n",
    "    labelCol='MOTHER_WEIGHT_GAIN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[\n",
    "        featuresCreator, \n",
    "        selector,\n",
    "        regressor])\n",
    "\n",
    "weightGain = pipeline.fit(births_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.487908174366\n"
     ]
    }
   ],
   "source": [
    "evaluator = ev.RegressionEvaluator(\n",
    "    predictionCol=\"prediction\", \n",
    "    labelCol='MOTHER_WEIGHT_GAIN')\n",
    "\n",
    "print(evaluator.evaluate(\n",
    "     weightGain.transform(births_test), \n",
    "    {evaluator.metricName: 'r2'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark (Spark 2.1.0, 100 executors)",
   "language": "python",
   "name": "pyspark27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
